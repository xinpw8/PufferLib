[base]
package = ocean
env_name = puffer_cartpole
vec = native
policy_name = Policy
rnn_name = Recurrent

[env]
num_envs = 512

[train]
anneal_lr = True
batch_size = 16384
bptt_horizon = 8
checkpoint_interval = 50
clip_coef = 0.2
clip_vloss = True
compile = False
compile_mode = reduce-overhead
cpu_offload = False
data_dir = experiments
device = cuda
ent_coef = 0.00003900277311898606
env_batch_size = 1
gae_lambda = 0.8175218896853491
gamma = 0.95
learning_rate = 0.00025
max_grad_norm = 1.877352899116142
minibatch_size = 256
norm_adv = True
num_envs = 1
num_workers = 1
seed = 1
torch_deterministic = True
total_timesteps = 13000000
update_epochs = 1
vf_clip_coef = 0.2
vf_coef = 0.9341430639021528
zero_copy = True
optimizer = adam
use_puff_advantage = False

[sweep]
method = protein
name = sweep

[sweep.metric]
goal = maximize
name = episode_length
min = 0
max = 205

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e6
max = 1e7
mean = 5e6
scale = 0.5

[sweep.train.gamma]
distribution = log_normal
min = 0.9
max = 0.999
mean = 0.97

[sweep.train.gae_lambda]
distribution = log_normal
min = 0.7
max = 0.999
mean = 0.95

[sweep.train.learning_rate]
distribution = log_normal
min = 0.0001
max = 0.001
mean = 0.00025
scale = 0.5

[sweep.train.batch_size]
min = 32768
max = 131072
mean = 65536
scale = 0.5

[sweep.train.minibatch_size]
min = 512
max = 2048
mean = 1024
scale = 0.5
